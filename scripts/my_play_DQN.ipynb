{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import textworld\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tree import Tree\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.9 #0.995\n",
    "        self.learning_rate = 0.5 #0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size)) #, activation='relu'\n",
    "        #model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='sigmoid')) #linear\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, targets_f = [], []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target \n",
    "            # Filtering out states and targets for training\n",
    "            states.append(state[0])\n",
    "            targets_f.append(target_f[0])\n",
    "        history = self.model.fit(np.array(states), np.array(targets_f), epochs=1, verbose=0)\n",
    "        # Keeping track of loss\n",
    "        loss = history.history['loss'][0]\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return loss\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_room(desc):\n",
    "    \n",
    "    room = ''\n",
    "    lines = desc\n",
    "    lines = lines.lower()\n",
    "    lines = lines.split('\\n')\n",
    "    line = lines[0]\n",
    "    line = line.strip()\n",
    "    if \"-=\" in line and \"=-\" in line:\n",
    "        room = line.split(\" \")[1]\n",
    "    \n",
    "    return room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertStateTextToStateValue(state):\n",
    "    global all_state_keys,state_size\n",
    "    \n",
    "    #5個房間\n",
    "    room = ['scullery', 'attic', 'bedchamber', 'pantry', 'vault']\n",
    "    \n",
    "    all_state_vals = []\n",
    "    \n",
    "    current_room = get_room(state)\n",
    "    for r in room:\n",
    "        if current_room == r:\n",
    "            v = 1\n",
    "        else:\n",
    "            v = 0\n",
    "        all_state_vals.append(v)\n",
    "    \n",
    "    return all_state_vals\n",
    "\n",
    "def convertActionIndexToActionValue(action_index):\n",
    "    #28個action\n",
    "    action = ['take formless keycard', 'go east', 'insert passkey into locker', 'take passkey from locker', 'go west', 'insert formless keycard into locker', 'take formless keycard from locker', 'go south', 'insert cloak into toolbox', 'take key from toolbox', 'insert keycard into toolbox', 'go north', 'insert lampshade into locker', 'take lampshade from locker', 'insert formless keycard into toolbox', 'insert lampshade into toolbox', 'take formless keycard from toolbox', 'insert key into toolbox', 'take cloak from toolbox', 'take keycard from toolbox', 'insert passkey into toolbox', 'insert cloak into locker', 'insert keycard into locker', 'take cloak from locker', 'take passkey from toolbox', 'take lampshade from toolbox', 'take keycard from locker', 'insert key into locker']\n",
    "    return action[action_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. steps: 1000.0; avg. score:  0.0 / 1.\n"
     ]
    }
   ],
   "source": [
    "state_size = 5\n",
    "action_size = 28\n",
    "agent = DQNAgent(state_size,action_size)\n",
    "done = False\n",
    "batch_size = 32\n",
    "\n",
    "step_total = 1000\n",
    "step_count = 0\n",
    "\n",
    "env = textworld.start(\"gen_games/tw-game-vjs3cos0-house-GP-OgOJFl9Jtba5I1Rb.ulx\")\n",
    "tw = textworld.agents.NaiveAgent()  # Or your own `textworld.Agent` subclass.\n",
    "\n",
    "env.activate_state_tracking()\n",
    "env.compute_intermediate_reward()\n",
    "env.enable_extra_info(\"description\")\n",
    "env.enable_extra_info(\"inventory\")\n",
    "    \n",
    "avg_moves, avg_scores = [], []\n",
    "EPISODES = 10\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    tw.reset(env)  # Tell the agent a new episode is starting.\n",
    "    game_state = env.reset()  # Start new episode.\n",
    "    \n",
    "    #state = env.reset()\n",
    "    #state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    reward = 0\n",
    "    done = False\n",
    "    \n",
    "    for time in range(step_total):\n",
    "        \n",
    "        room = get_room(game_state.description)\n",
    "        state = convertStateTextToStateValue(room)\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        \n",
    "        #command = get_good_command(game_state.admissible_commands)\n",
    "        action = agent.act(state)\n",
    "        command = convertActionIndexToActionValue(action)\n",
    "        game_state, reward, done = env.step(command)\n",
    "        \n",
    "        im_reward = game_state.intermediate_reward\n",
    "        \n",
    "        next_room = get_room(game_state.description)\n",
    "        next_state = convertStateTextToStateValue(next_room)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        \n",
    "        agent.remember(state, action, im_reward, next_state, done)\n",
    "        #state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print('======COMPLETED======')\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, EPISODES, time, agent.epsilon))\n",
    "            break\n",
    "            \n",
    "        if len(agent.memory) > batch_size:\n",
    "            loss = agent.replay(batch_size)\n",
    "            # Logging training loss every 10 timesteps\n",
    "            #if time % 10 == 0:\n",
    "            #    print(\"episode: {}/{}, time: {}, loss: {:.4f}\".format(e, EPISODES, time, loss))         \n",
    "    # See https://textworld-docs.maluuba.com/textworld.html#textworld.core.GameState\n",
    "    avg_moves.append(game_state.nb_moves)\n",
    "    avg_scores.append(game_state.score)\n",
    "\n",
    "env.close()\n",
    "print(\"avg. steps: {:5.1f}; avg. score: {:4.1f} / 1.\".format(sum(avg_moves)/EPISODES, sum(avg_scores)/EPISODES))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
